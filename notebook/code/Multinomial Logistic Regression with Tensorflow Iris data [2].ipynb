{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Network[ 4 Layers Neural Network for Iris Data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.core.pylabtools import figsize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "root = r'C:\\\\Users\\\\admin\\\\Desktop\\\\Python_Prog\\\\PyMC3\\\\Styles\\\\bmh_matplotlibrc.json'\n",
    "s = json.load(open(root))\n",
    "matplotlib.rcParams.update(s)\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "iris = pd.read_csv(\"C:\\\\Users\\\\admin\\\\Desktop\\\\Deep Learning\\\\data\\\\iris.csv\", names = headers)\n",
    "\n",
    "iris.sepal_length =(( iris.sepal_length  - iris.sepal_length.mean())/ iris.sepal_length.std()).astype(float)\n",
    "iris.sepal_width =(( iris.sepal_width  - iris.sepal_width.mean())/ iris.sepal_width.std()).astype(float)\n",
    "iris.petal_length =(( iris.petal_length - iris.petal_length.mean())/ iris.petal_length.std()).astype(float)\n",
    "iris.sepal_length =(( iris.petal_width  - iris.petal_width.mean())/ iris.petal_width.std()).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split our data into train, validation and test sets\n",
    "#before split always remenber to chech if the class are well balance\n",
    "np.random.seed(1) #for debugging we want to have the same state\n",
    "shuffled_index = np.random.permutation(iris.index)\n",
    "shuffled_iris = iris.loc[shuffled_index]\n",
    "\n",
    "shuffled_iris.species = (shuffled_iris.species == 'Iris-versicolor').values.astype(int)\n",
    "shuffled_iris.loc[:, ('not_species')] = shuffled_iris['species'] == 0\n",
    "shuffled_iris.loc[:, ('not_species')] = shuffled_iris['not_species'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = shuffled_iris.loc[:, ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].as_matrix()\n",
    "y = shuffled_iris.loc[:, ['species', 'not_species']].as_matrix()\n",
    "trainX, trainY = X[:50, :], y[:50, :]\n",
    "validX, validY = X[50:100], y[50:100]\n",
    "testX, testY = X[100:], y[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return 100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_units_1 = 1024\n",
    "hidden_units_2 = 1024\n",
    "batch_size = 40\n",
    "Lambda = 1e-5\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, X.shape[1]))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, y.shape[1]))\n",
    "    tf_valid_dataset = tf.placeholder(tf.float32, shape=(validX.shape))\n",
    "    tf_test_dataset = tf.placeholder(tf.float32, shape=(testX.shape))\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    #Variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([X.shape[1], hidden_units_1], \n",
    "                                               stddev = np.sqrt(2.0 / X.shape[1])))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_units_1]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_units_1, hidden_units_2],\n",
    "                                               stddev = np.sqrt(2.0 / hidden_units_1)))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden_units_2]))\n",
    "    weights3 = tf.Variable(tf.truncated_normal([hidden_units_2, y.shape[1]],\n",
    "                                               stddev = np.sqrt(2.0 / hidden_units_2)))\n",
    "    biases3 = tf.Variable(tf.zeros([y.shape[1]]))\n",
    "    \n",
    "    #Training\n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "    logits = tf.matmul(hidden2, weights3) + biases3\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = tf_train_labels, logits = logits))\\\n",
    "    + (Lambda/(2*batch_size))*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "    \n",
    "    #Optmization\n",
    "    learning_rate = tf.train.exponential_decay(0.8, global_step, 4000, 0.6, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    \n",
    "    #Predictions\n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    hidden_valid_1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    hidden_valid_2 = tf.nn.relu(tf.matmul(hidden_valid_1, weights2) + biases2)\n",
    "    valid_predictions = tf.nn.softmax(tf.matmul(hidden_valid_2, weights3) + biases3)\n",
    "    hidden_test_1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    hidden_test_2 = tf.nn.relu(tf.matmul(hidden_test_1, weights2) + biases2)\n",
    "    test_predictions = tf.nn.softmax(tf.matmul(hidden_test_2, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at 0: 0.677087128162384\n",
      "Minibatch accuray : 57.5%\n",
      "Validation accuracy: 46.0%\n",
      "Minibatch loss at 1000: 0.0012525484198704362\n",
      "Minibatch accuray : 100.0%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at 2000: 0.0012229284038767219\n",
      "Minibatch accuray : 100.0%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at 3000: 0.0012144912034273148\n",
      "Minibatch accuray : 100.0%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at 4000: 0.0012108426308259368\n",
      "Minibatch accuray : 100.0%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at 5000: 0.001209388137795031\n",
      "Minibatch accuray : 100.0%\n",
      "Validation accuracy: 92.0%\n",
      "Test accuracy: 96.0%\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "num_steps = 5001\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in np.arange(num_steps):\n",
    "        offset = (step * batch_size) % (trainY.shape[0]- batch_size)\n",
    "        batch_data = trainX[offset:(offset + batch_size), :]\n",
    "        batch_labels = trainY[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_predictions], feed_dict = feed_dict)\n",
    "        if step%1000 == 0:\n",
    "            print(\"Minibatch loss at {0}: {1}\".format(step, l))\n",
    "            print(\"Minibatch accuray : %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                        valid_predictions.eval(feed_dict={tf_valid_dataset:validX}), validY))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_predictions.eval(feed_dict={tf_test_dataset:testX}), \n",
    "                                             testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
