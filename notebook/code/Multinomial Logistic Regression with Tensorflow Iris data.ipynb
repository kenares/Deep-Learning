{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression with Tensorflow Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.core.pylabtools import figsize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "root = r'C:\\\\Users\\\\admin\\\\Desktop\\\\Python_Prog\\\\PyMC3\\\\Styles\\\\bmh_matplotlibrc.json'\n",
    "s = json.load(open(root))\n",
    "matplotlib.rcParams.update(s)\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width         species\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "iris = pd.read_csv(\"C:\\\\Users\\\\admin\\\\Desktop\\\\Deep Learning\\\\data\\\\iris.csv\", names = headers)\n",
    "iris.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.sepal_length =(( iris.sepal_length  - iris.sepal_length.mean())/ iris.sepal_length.std()).astype(float)\n",
    "iris.sepal_width =(( iris.sepal_width  - iris.sepal_width.mean())/ iris.sepal_width.std()).astype(float)\n",
    "iris.petal_length =(( iris.petal_length - iris.petal_length.mean())/ iris.petal_length.std()).astype(float)\n",
    "iris.sepal_length =(( iris.petal_width  - iris.petal_width.mean())/ iris.petal_width.std()).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split our data into train, validation and test sets\n",
    "#before split always remenber to chech if the class are well balance\n",
    "shuffled_index = np.random.permutation(iris.index)\n",
    "shuffled_iris = iris.loc[shuffled_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffled_iris.species = (shuffled_iris.species == 'Iris-versicolor').values.astype(int)\n",
    "shuffled_iris.loc[:, ('not_species')] = shuffled_iris['species'] == 0\n",
    "shuffled_iris.loc[:, ('not_species')] = shuffled_iris['not_species'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tensor are a genric version of vector and matrix\n",
    "#A list is  1D tensor\n",
    "#A list of list is a matrix, a matrix is a 2D tensor\n",
    "#A list of list of list is then a 3D tensor\n",
    "\n",
    "X = shuffled_iris.loc[:, ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].as_matrix()\n",
    "y = shuffled_iris.loc[:, ['species', 'not_species']].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX, trainY = X[:50, :], y[:50, :]\n",
    "validX, validY = X[50:100], y[50:100]\n",
    "testX, testY = X[100:], y[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return 100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SGD\n",
    "batch_size = 40\n",
    "Lambda = 1e-5\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, X.shape[1]))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, y.shape[1]))\n",
    "    tf_validation_dataset = tf.placeholder(tf.float32, shape = (validX.shape))\n",
    "    tf_test_dataset = tf.placeholder(tf.float32, shape = (testX.shape))\n",
    "    \n",
    "    #Variables\n",
    "    weights = tf.Variable(tf.truncated_normal([X.shape[1], y.shape[1]]))\n",
    "    biases = tf.Variable(tf.zeros([y.shape[1]]))\n",
    "    \n",
    "    #Training\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels = tf_train_labels, logits = logits)) \\\n",
    "        + (Lambda/2*batch_size)*(tf.nn.l2_loss(weights))\n",
    "    \n",
    "    #Optimization\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    #Predctions\n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    valid_predictions = tf.nn.softmax(tf.matmul(tf_validation_dataset, weights) + biases)\n",
    "    test_predictions = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0 : 1.3457576036453247\n",
      "Training accuracy : 40.0\n",
      "Validation accuracy : 38.0\n",
      "Loss at step 500 : 0.42070624232292175\n",
      "Training accuracy : 82.5\n",
      "Validation accuracy : 78.0\n",
      "Loss at step 1000 : 0.4203040599822998\n",
      "Training accuracy : 82.5\n",
      "Validation accuracy : 76.0\n",
      "Loss at step 1500 : 0.42024701833724976\n",
      "Training accuracy : 82.5\n",
      "Validation accuracy : 76.0\n",
      "Loss at step 2000 : 0.42021632194519043\n",
      "Training accuracy : 82.5\n",
      "Validation accuracy : 76.0\n",
      "Loss at step 2500 : 0.4201897084712982\n",
      "Training accuracy : 82.5\n",
      "Validation accuracy : 76.0\n",
      "Loss at step 3000 : 0.4201652705669403\n",
      "Training accuracy : 82.5\n",
      "Validation accuracy : 76.0\n",
      "Test accuracy : 74.0\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in np.arange(num_steps):\n",
    "        offset = (step * batch_size) % (trainY.shape[0] - batch_size)\n",
    "        batch_data = trainX[offset : (offset + batch_size), :]\n",
    "        batch_labels = trainY[offset : (offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels,\n",
    "                                    tf_validation_dataset:validX, tf_test_dataset:testX}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_predictions], feed_dict = feed_dict)\n",
    "        if step%500 == 0:\n",
    "            print('Loss at step {0} : {1}'.format(step, l))\n",
    "            print('Training accuracy : %.1f'%accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy : %.1f'%accuracy(valid_predictions.eval(\n",
    "                        feed_dict = {tf_validation_dataset:validX}), validY))\n",
    "    print('Test accuracy : %.1f'%accuracy(test_predictions.eval(\n",
    "                feed_dict = {tf_test_dataset:testX}), testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "72.0 to 78.0 after increasing the size of the batch from 30 to 40\n",
    "but it seems our model is overfittinga bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_graph = tf.Graph()\n",
    "hidden_units = 1024\n",
    "batch_size = 40\n",
    "Lambda = 1e-5\n",
    "with _graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, X.shape[1]))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, y.shape[1]))\n",
    "    tf_validation_dataset = tf.placeholder(tf.float32, shape=(validX.shape))\n",
    "    tf_test_dataset = tf.placeholder(tf.float32, shape = (testX.shape))\n",
    "    \n",
    "    #variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([X.shape[1], hidden_units]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_units]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_units, y.shape[1]]))\n",
    "    biases2 = tf.Variable(tf.zeros([y.shape[1]]))\n",
    "    \n",
    "    #training\n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(hidden1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = tf_train_labels, logits = logits))\\\n",
    "    + (Lambda/(2*batch_size)) * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    \n",
    "    #Optimization\n",
    "    global_step = tf.Variable(0, trainable = False)\n",
    "    learning_rate = tf.train.exponential_decay(0.7, global_step, 100000, 0.96, staircase = True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    \n",
    "    #Predictions\n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    valid_hidden = tf.nn.relu(tf.matmul(tf_validation_dataset, weights1) + biases1)\n",
    "    valid_predictions = tf.nn.softmax(tf.matmul(valid_hidden, weights2) + biases2)\n",
    "    test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_predictions = tf.nn.softmax(tf.matmul(test_hidden, weights2)+ biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiliazed\n",
      "Minibatch error at step 0 : 21.402929306030273\n",
      "Minibatch accuracy  : 40.0%\n",
      "Validation accuracy : 66.0%\n",
      "Minibatch error at step 500 : 0.00043296514195390046\n",
      "Minibatch accuracy  : 100.0%\n",
      "Validation accuracy : 96.0%\n",
      "Minibatch error at step 1000 : 0.0004061236686538905\n",
      "Minibatch accuracy  : 100.0%\n",
      "Validation accuracy : 96.0%\n",
      "Minibatch error at step 1500 : 0.00038712029345333576\n",
      "Minibatch accuracy  : 100.0%\n",
      "Validation accuracy : 96.0%\n",
      "Minibatch error at step 2000 : 0.00037301916745491326\n",
      "Minibatch accuracy  : 100.0%\n",
      "Validation accuracy : 96.0%\n",
      "Minibatch error at step 2500 : 0.000361993967089802\n",
      "Minibatch accuracy  : 100.0%\n",
      "Validation accuracy : 96.0%\n",
      "Minibatch error at step 3000 : 0.0003529555397108197\n",
      "Minibatch accuracy  : 100.0%\n",
      "Validation accuracy : 96.0%\n",
      "Test accuracy : 92.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "with tf.Session(graph = _graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initiliazed')\n",
    "    for step in np.arange(num_steps):\n",
    "        offset = (step * batch_size)%(trainY.shape[0] - batch_size)\n",
    "        batch_data = trainX[offset:(offset + batch_size), :]\n",
    "        batch_labels = trainY[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_predictions], feed_dict = feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print('Minibatch error at step {0} : {1}'.format(step, l))\n",
    "            print('Minibatch accuracy  : %.1f%%'%accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy : %.1f%%'%accuracy(valid_predictions.eval(\n",
    "                        feed_dict = {tf_validation_dataset:validX}), validY))\n",
    "    print('Test accuracy : %.1f%%'%accuracy(test_predictions.eval(feed_dict = {tf_test_dataset:testX}), testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I achieved 94% accuracy with a 3 layer Neural Network without regularization\n",
    "# 98% with regularization nice work but the ai is to achieve 1 accuracy\n",
    "# let's add dropout before the readout layer : Dropout seems to reduce overfitting a lot but my accuracy went\n",
    "# down 92%, we will go without dropout fro iris data\n",
    "#Let's try learning rate decay : seems the model is not overfiting at all !\n",
    "# 92% on the validation set and 92% on the test set. the model peform better on the test set !!!! \n",
    "#(always implement learning rate decay seems effcient to reduce overfitting)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
