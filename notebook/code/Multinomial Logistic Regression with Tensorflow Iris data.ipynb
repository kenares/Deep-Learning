{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression with Tensorflow Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.core.pylabtools import figsize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "root = r'C:\\\\Users\\\\admin\\\\Desktop\\\\Python_Prog\\\\PyMC3\\\\Styles\\\\bmh_matplotlibrc.json'\n",
    "s = json.load(open(root))\n",
    "matplotlib.rcParams.update(s)\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width         species\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "iris = pd.read_csv(\"C:\\\\Users\\\\admin\\\\Desktop\\\\Deep Learning\\\\data\\\\iris.csv\", names = headers)\n",
    "iris.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.sepal_length =(( iris.sepal_length  - iris.sepal_length.mean())/ iris.sepal_length.std()).astype(float)\n",
    "iris.sepal_width =(( iris.sepal_width  - iris.sepal_width.mean())/ iris.sepal_width.std()).astype(float)\n",
    "iris.petal_length =(( iris.petal_length - iris.petal_length.mean())/ iris.petal_length.std()).astype(float)\n",
    "iris.sepal_length =(( iris.petal_width  - iris.petal_width.mean())/ iris.petal_width.std()).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split our data into train, validation and test sets\n",
    "#before split always remenber to chech if the class are well balance\n",
    "shuffled_index = np.random.permutation(iris.index)\n",
    "shuffled_iris = iris.loc[shuffled_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffled_iris.species = (shuffled_iris.species == 'Iris-versicolor').values.astype(int)\n",
    "shuffled_iris.loc[:, ('not_species')] = shuffled_iris['species'] == 0\n",
    "shuffled_iris.loc[:, ('not_species')] = shuffled_iris['not_species'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tensor are a genric version of vector and matrix\n",
    "#A list is  1D tensor\n",
    "#A list of list is a matrix, a matrix is a 2D tensor\n",
    "#A list of list of list is then a 3D tensor\n",
    "\n",
    "X = shuffled_iris.loc[:, ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].as_matrix()\n",
    "y = shuffled_iris.loc[:, ['species', 'not_species']].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX, trainY = X[:50, :], y[:50, :]\n",
    "validX, validY = X[50:100], y[50:100]\n",
    "testX, testY = X[100:], y[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return 100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SGD\n",
    "batch_size = 40\n",
    "Lambda = 1e-5\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, X.shape[1]))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, y.shape[1]))\n",
    "    tf_validation_dataset = tf.placeholder(tf.float32, shape = (validX.shape))\n",
    "    tf_test_dataset = tf.placeholder(tf.float32, shape = (testX.shape))\n",
    "    \n",
    "    #Variables\n",
    "    weights = tf.Variable(tf.truncated_normal([X.shape[1], y.shape[1]]))\n",
    "    biases = tf.Variable(tf.zeros([y.shape[1]]))\n",
    "    \n",
    "    #Training\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels = tf_train_labels, logits = logits)) \\\n",
    "        + (Lambda/2*batch_size)*(tf.nn.l2_loss(weights))\n",
    "    \n",
    "    #Optimization\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    #Predctions\n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    valid_predictions = tf.nn.softmax(tf.matmul(tf_validation_dataset, weights) + biases)\n",
    "    test_predictions = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0 : 1.9838948249816895\n",
      "Training accuracy : 57.5\n",
      "Validation accuracy : 58.0\n",
      "Loss at step 150 : 0.46138274669647217\n",
      "Training accuracy : 70.0\n",
      "Validation accuracy : 66.0\n",
      "Loss at step 300 : 0.458565890789032\n",
      "Training accuracy : 75.0\n",
      "Validation accuracy : 70.0\n",
      "Loss at step 450 : 0.45793092250823975\n",
      "Training accuracy : 77.5\n",
      "Validation accuracy : 70.0\n",
      "Loss at step 600 : 0.45777374505996704\n",
      "Training accuracy : 77.5\n",
      "Validation accuracy : 70.0\n",
      "Loss at step 750 : 0.45772701501846313\n",
      "Training accuracy : 77.5\n",
      "Validation accuracy : 70.0\n",
      "Loss at step 900 : 0.4577067792415619\n",
      "Training accuracy : 77.5\n",
      "Validation accuracy : 72.0\n",
      "Test accuracy : 78.0\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in np.arange(num_steps):\n",
    "        offset = (step * batch_size) % (trainY.shape[0] - batch_size)\n",
    "        batch_data = trainX[offset : (offset + batch_size), :]\n",
    "        batch_labels = trainY[offset : (offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels,\n",
    "                                    tf_validation_dataset:validX, tf_test_dataset:testX}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_predictions], feed_dict = feed_dict)\n",
    "        if step%150 == 0:\n",
    "            print('Loss at step {0} : {1}'.format(step, l))\n",
    "            print('Training accuracy : %.1f'%accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy : %.1f'%accuracy(valid_predictions.eval(\n",
    "                        feed_dict = {tf_validation_dataset:validX}), validY))\n",
    "    print('Test accuracy : %.1f'%accuracy(test_predictions.eval(\n",
    "                feed_dict = {tf_test_dataset:testX}), testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "72.0 to 78.0 after increasing the size of the batch from 30 to 40\n",
    "but it seems our model is overfittinga bit."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
